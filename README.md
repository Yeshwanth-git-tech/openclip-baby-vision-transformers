
# ðŸ§  Egocentric Video Understanding using OpenCLIP on Infant-Homeview Dataset

This project is part of an applied research initiative at **Indiana University Luddy School of Informatics**. It explores contrastive image-language modeling over **100,000+ hours of egocentric baby videos** captured in real-world home environments.

---

## ðŸ“¦ Dataset Overview

> **Note**: This is *not the standard SAYCam dataset*. The dataset was provided as part of an internal IU research project.

- **Source**: Raw egocentric videos (infant headcam)
- **Duration**: ~100k+ hours of footage
- **Processing Pipeline**:
  - ðŸŽžï¸ Frame extraction using `ffmpeg` (1 FPS to 5 FPS)
  - ðŸ”Š Audio transcription using `Whisper AI` (OpenAI) to generate weak supervision labels
  - ðŸ§Š Tar-based storage using `WebDataset` format (sharded to ~4,000 `.tar` files)
  - ðŸ¤– Sample format: `{image, text}` pairs per frame

---

## âš™ï¸ Technical Infrastructure

- **Compute**: [Big Red 200 Supercomputer](https://kb.iu.edu/d/avjq) (IU HPC cluster)
- **GPU**: NVIDIA A100 (Ã—4) across multiple SLURM jobs
- **Storage**: Lustre parallel filesystem (15+ TB raw data)
- **Job Management**: SLURM (`sbatch`, `srun`, `torchrun`)

---

## ðŸ› ï¸ Pretraining Pipeline

- âœ… Custom data loaders for `WebDataset` format  
- âœ… Multi-GPU distributed training via `torchrun`  
- âœ… Mixed precision training with AMP (`--precision amp`)  
- âœ… Model logging + monitoring via `Weights & Biases`  
- âœ… Validation loss graph and Clip Loss tracking implemented

---

## ðŸ” Models Trained

We trained **multiple OpenCLIP backbones** end-to-end on the Infant-Homeview dataset:

| Model Name     | Command Flag         | Parameters    |
|----------------|----------------------|---------------|
| ResNet-50      | `--model RN50`       | 102M          |
| ResNet-101     | `--model RN101`      | 130M          |
| ViT-B/32       | `--model ViT-B-32`   | 151M          |
| ViT-B/16       | `--model ViT-B-16`   | 151M          |
| ViT-L/14       | `--model ViT-L-14`   | 428M          |

â±ï¸ We used **gradual scaling** based on GPU availability and convergence trends. Results improved with larger backbones on the domain-specific egocentric dataset.

---

## ðŸ“ˆ Key Achievements

- ðŸ”„ Complete training-validation split over 4M image-text pairs
- ðŸ”¥ Achieved stable Clip Loss reductions with `ViT-B-32` and `ViT-L-14`
- ðŸŽ¯ Validated on unseen real-world baby-view frames (custom benchmark)
- ðŸ“‰ Loss convergence observed at scale (see W&B graphs)
- ðŸ§ª Verified image-text matching via inference on unseen samples
- ðŸ§  Grounded text captions from Whisper-based pseudo-labels

---

## ðŸ§¾ Citation / Acknowledgements

This project builds upon:

- [OpenCLIP](https://github.com/mlfoundations/open_clip)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [WebDataset](https://github.com/webdataset/webdataset)
- Indiana University HPC Resources

> All data and experiments are conducted under ethical research guidelines and internal research agreements.

---

## ðŸ“ Repository Structure

```bash
â”œâ”€â”€ open_clip_train/       # Custom scripts with val loss, batch logs etc.
â”œâ”€â”€ data_pipeline/         # FFmpeg + Whisper preprocessing scripts
â”œâ”€â”€ job_scripts/           # SLURM sbatch scripts (bash)
â”œâ”€â”€ logs/                  # W&B or local logs
â”œâ”€â”€ README.md              # This file
â””â”€â”€ clipgpu_test1.py       # CLIP inference sanity check
```

---

## ðŸ§© Repository Strategy

Since this work builds on `open_clip`, but your fork was from `openai/CLIP`, we recommend:

> âœ… **Create a new GitHub repo** (e.g., `openclip-homeview-egocentric`)  
> âœ… Push your full working codebase there  
> âœ… Include logs, visuals, and checkpoints selectively

This will clarify your contribution and distinguish your repo from the older OpenAI CLIP release.

---

## ðŸš€ Future Steps

- Add `inference.py` to evaluate your trained checkpoints
- Push sample `.tar` files (5â€“10) to test reproducibility
- Optional: Export graphs or logs from W&B for results

---

### Author: Yeshwanth Satheesh 
